{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import uszipcode\n",
    "import zipcode\n",
    "import us\n",
    "import pickle\n",
    "import string\n",
    "import timeit\n",
    "os.environ['ZIPCODE_CONNECTION_STRING'] = 'sqlite:////home/modp/zipcode.db'\n",
    "\n",
    "ads_file = 'us-ads.csv'\n",
    "cities_file = 'us-cities/uscities.csv'\n",
    "all_regions_list = pickle.load(open( \"all_regions.p\", \"rb\" ) )\n",
    "\n",
    "county_fips_file = 'geo-data/county_fips.csv'\n",
    "cg_fips_file = 'geo-data/US_Congress_FIPS.csv'\n",
    "\n",
    "# fips_file = 'geo-data/ZIP-COUNTY-FIPS_2018-03.csv'\n",
    "fips_file = 'US_FIPS.csv'\n",
    "district_fips_file = 'district-fips.csv'\n",
    "\n",
    "# County,State,USA -> County, State\n",
    "county_st_us = re.compile('([A-Za-z]+\\sCounty),(\\w{2}),(USA)')\n",
    "\n",
    "county_state_us_regex = re.compile(r'\\b([A-Za-z]+\\sCounty),(\\w{2}),USA\\b')\n",
    "county_regex = re.compile(r'\\b([A-Za-z]+\\s?[A-Za-z]+)(\\sCounty)\\b')\n",
    "city_regex = re.compile(r'\\b([A-Za-z]+\\s?[A-Za-z]{3,})\\b')\n",
    "city_state_regex = re.compile(r'\\b([A-Za-z]+\\s?[A-Za-z]{3,}),\\s+([A-Z]{2})\\b')\n",
    "\n",
    "district_regex= re.compile(r'\\b([A-Z]{2}-\\d{1,2})(,[A-Z]{2},USA)\\b')\n",
    "\n",
    "cg_regex = re.compile(r'\\b([A-Z]{2})(-)(\\d{1,2})\\b')\n",
    "only_zips = re.compile(r'\\b^[0-9]+|,$\\b')\n",
    "zip_regex = re.compile(r'\\b\\d{5}\\b')\n",
    "state_regex = re.compile(r'\\b[A-Z]{2}\\b')\n",
    "geotarget = 'Geo_Targeting_Included'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# district_fips.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# district_fips.to_csv('district-fips.csv', index=False)\n",
    "\n",
    "fips = pd.read_csv(fips_file, dtype=str)\n",
    "district_fips = pd.read_csv('district-fips.csv', dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_old(df):\n",
    "    # Change 'United States' to 'USA', abbreviate state names\n",
    "    for idx,row in  df.iterrows():\n",
    "        locations = row[geotarget]\n",
    "        # Create two sets \n",
    "        # First set catches { Arizona, Georgia, Idaho, ... }\n",
    "        # Second set catches { AZ-1,Arizona,United States, ... }\n",
    "        state_matches = us_states & set(locations.split(',')) | us_states & set(locations.split(', '))\n",
    "        for match in state_matches:\n",
    "            locations = locations.replace(match, state2abbr[match])\n",
    "#         locations = locations.replace(re.escape('WA, DC (Hagerstown, Maryland), WA'), \"Hangerstown, MD\")\n",
    "        df.at[idx, geotarget] = locations\n",
    "\n",
    "    # Simplify district name information\n",
    "    # e.g: AZ-1,AZ,USA -> AZ-1\n",
    "    df[geotarget] = df[geotarget].str.replace(district_regex, r'\\1')\n",
    "    df.Geo_Targeting_Included = df.Geo_Targeting_Included.str.replace(county_st_us,r'\\1, \\2' )\n",
    "\n",
    "    special_cases = {\n",
    "        'George s': \"George's\",\n",
    "        'Raleigh-Durham (Fayetteville), NC' : 'Fayetteville, NC',\n",
    "        'WA, DC (Hagerstown, Maryland), WA' : \"\" ,\n",
    "        'WA, DC (Hagerstown, MD), WA' : \"Hagerstown, MD\" ,\n",
    "        'Richmond-Petersburg, VA' : 'Richmond, VA, Petersburg, VA',\n",
    "        'Harrisburg-Lancaster-York, PA' : 'Cumberland County, Dauphin County, Perry County, Lebanon County, Adams County, York County'\n",
    "    }\n",
    "    for case in special_cases:\n",
    "        df[geotarget] = df[geotarget].str.replace(re.escape(case), special_cases[case])\n",
    "    df[geotarget] = df[geotarget].str.replace('George s',\"George's\")\n",
    "   \n",
    "    return df\n",
    "\n",
    "def cleanup_states(data):\n",
    "    state_matches = us_states & set(data.split(',')) | us_states & set(data.split(', '))\n",
    "    for match in state_matches:\n",
    "        data = data.replace(match, state2abbr[match])   \n",
    "    return data \n",
    "\n",
    "def cleanup(data):\n",
    "    # Change 'United States' to 'USA', abbreviate state names\n",
    "        # Create two sets \n",
    "        # First set catches { Arizona, Georgia, Idaho, ... }\n",
    "        # Second set catches { AZ-1,Arizona,United States, ... }\n",
    "        \n",
    "#         state_matches = us_states & set(data.split(',')) | us_states & set(data.split(', '))\n",
    "#         for match in state_matches:\n",
    "#             data = data.replace(match, state2abbr[match])\n",
    "\n",
    "\n",
    "    # Simplify district name information\n",
    "    # e.g: AZ-1,AZ,USA -> AZ-1\n",
    "    data[geotarget] = data[geotarget].str.replace(district_regex, r'\\1')\n",
    "    data[geotarget] = data[geotarget].str.replace(county_st_us,r'\\1, \\2' )\n",
    "\n",
    "    special_cases = {\n",
    "        'George s': \"George's\",\n",
    "        'Raleigh-Durham (Fayetteville), NC' : 'Fayetteville, NC',\n",
    "        'WA, DC (Hagerstown, Maryland), WA' : \"\" ,\n",
    "        'WA, DC (Hagerstown, MD), WA' : \"Hagerstown, MD\" ,\n",
    "        'Richmond-Petersburg, VA' : 'Richmond, VA, Petersburg, VA',\n",
    "        'Harrisburg-Lancaster-York, PA' : 'Cumberland County, Dauphin County, Perry County, Lebanon County, Adams County, York County'\n",
    "    }\n",
    "    for case in special_cases:\n",
    "        data[geotarget] = data[geotarget].str.replace(re.escape(case), special_cases[case])\n",
    "#     df[geotarget] = df[geotarget].str.replace('George s',\"George's\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_state_dict():\n",
    "    \"\"\"\n",
    "    Make dictionaries that map state names and abbreviations\n",
    "    e.g. Iowa -> IA, IA -> Iowa\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    state2abbr = {}\n",
    "    abbr2state = {}\n",
    "    for state in us.states.STATES:\n",
    "        state2abbr[state.name] = state.abbr\n",
    "        abbr2state[state.abbr] = state.name\n",
    "    state2abbr['United States'] = 'USA'\n",
    "    abbr2state['US'] = 'United States'\n",
    "    return state2abbr, abbr2state\n",
    "\n",
    "state2abbr, abbr2state = make_state_dict()\n",
    "us_states = [s.name for s in us.states.STATES]\n",
    "us_states = set(us_states)\n",
    "us_states.add('United States')\n",
    "c_districts = pickle.load(open( \"cg_zips.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('/home/modp/zipcode.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# cursor.description\n",
    "cursor.execute('''SELECT DISTINCT city, state from zipcodes''')\n",
    "city_state = cursor.fetchall()\n",
    "\n",
    "cursor.execute('''SELECT county from zipcodes''')\n",
    "counties = cursor.fetchall()\n",
    "\n",
    "cursor.execute('''SELECT county, state from zipcodes''')\n",
    "counties_state = cursor.fetchall()\n",
    "\n",
    "cursor.execute('''SELECT DISTINCT city from zipcodes''')\n",
    "cities = cursor.fetchall()\n",
    "\n",
    "cursor.execute('''SELECT state from zipcodes''')\n",
    "states = cursor.fetchall()\n",
    "\n",
    "cursor.execute('''SELECT zipcode from zipcodes''')\n",
    "all_zipcodes = cursor.fetchall()\n",
    "\n",
    "\n",
    "city_state_set = [', '.join(c) for c in city_state]\n",
    "counties_set  = [', '.join(c) for c in counties]\n",
    "counties_state_set  = [', '.join(c) for c in counties_state]\n",
    "zipcodes_set = [', '.join(z) for z in all_zipcodes]\n",
    "cities_set = [', '.join(c) for c in cities]\n",
    "state_abbr_set = [', '.join(s) for s in states]\n",
    "state_names_set = [state.name for state in us.states.STATES]\n",
    "\n",
    "district_regions_set = []\n",
    "for cd in c_districts:\n",
    "    for d in c_districts[cd]: \n",
    "        district_regions_set.append(cd + '-' +str(d))\n",
    "\n",
    "# all_regions = city_state_set + counties_set + counties_state_set + cities_set + state_names_set + state_abbr_set + district_regions_set\n",
    "# pickle.dump( all_regions, open( \"all_regions.p\", \"wb\" ) )\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dictionary mapping regions to FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a CITYSTATE column e.g. Austin, TX\n",
    "fips['CITYSTATE'] = fips['CITY'] + ', ' + fips['STATE']\n",
    "\n",
    "# Create a COUNTRYSTATE column: e.g. King's County, NY\n",
    "fips['COUNTYSTATE'] = fips['COUNTY'] + ', ' + fips['STATE']\n",
    "\n",
    "# County/State dictionary \n",
    "fips_countystate = fips.set_index('COUNTYSTATE')\n",
    "countystate_dict = fips_countystate['FIPS'].to_dict()\n",
    "\n",
    "# City/State dictionary \n",
    "fips_city = fips.set_index('CITYSTATE')\n",
    "citystate_dict = fips_city['FIPS'].to_dict()\n",
    "\n",
    "# Zipcode dictionary\n",
    "f = fips.set_index('ZIP')\n",
    "zip2fips = f['FIPS'].to_dict()\n",
    "\n",
    "# District dictionary\n",
    "d = district_fips.set_index('DISTRICT')\n",
    "district2fips = d['FIPS'].to_dict()\n",
    "\n",
    "# Create dict for counties e.g. state_fips['King's County'] = [fips1, fips2, ... ]\n",
    "e = fips[['COUNTY', 'FIPS']].set_index('COUNTY')\n",
    "county_fips = e.groupby('COUNTY').apply(lambda dfg: dfg.to_dict(orient='list')).to_dict()\n",
    "new_f = {}\n",
    "for county in county_fips:\n",
    "    new_f[county] = list(set(county_fips[county]['FIPS']))\n",
    "county2fips = new_f \n",
    "\n",
    "# Create dict for states e.g. state_fips['AK'] = [fips1, fips2, ... ]\n",
    "d = fips[['STATE', 'FIPS']].set_index('STATE')\n",
    "state_fips = d.groupby('STATE').apply(lambda dfg: dfg.to_dict(orient='list')).to_dict()\n",
    "new_f = {}\n",
    "for state in state_fips:\n",
    "    new_f[state] = list(set(state_fips[state]['FIPS']))\n",
    "state2fips = new_f    \n",
    "\n",
    "\n",
    "# Merge all dictionaries together \n",
    "fips_dict = {**state2fips, **county2fips, **district2fips, **zip2fips, **citystate_dict, **countystate_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords\n",
    "\n",
    "Create three keyword processors. One for just cities, one for states, and one for everything else. The idea is to sift through the geo-targeting data and remove everything but lone cities and states first.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keyword processor for everything but the \n",
    "kwd = KeywordProcessor()\n",
    "kwd_cities = KeywordProcessor()\n",
    "kwd_states = KeywordProcessor()\n",
    "\n",
    "\n",
    "# for cg_dist in district_fips.DISTRICT:\n",
    "#     kwd.add_keyword(city_state, ('District', cg_dist))\n",
    "\n",
    "for dist in district_regions_set:\n",
    "    kwd.add_keyword(dist, ('District', dist))\n",
    "    \n",
    "for county in counties_set:\n",
    "    kwd.add_keyword(county, ('County', county))\n",
    "    \n",
    "for county in counties_state_set:\n",
    "    kwd.add_keyword(county, ('County', county))\n",
    "    \n",
    "    \n",
    "for z in zipcodes_set:\n",
    "    kwd.add_keyword(z, ('Zipcode', z))\n",
    "    \n",
    "for city_state in city_state_set:\n",
    "    kwd.add_keyword(city_state, ('City/State', city_state))\n",
    "    \n",
    "for city in cities_set:\n",
    "    kwd_cities.add_keyword(city, ('City', city))\n",
    "    \n",
    "for state in state_abbr_set:\n",
    "    kwd_states.add_keyword(state, ('State', state))\n",
    "    \n",
    "kwd_replace = KeywordProcessor()\n",
    "for k in kwd.get_all_keywords().keys():\n",
    "    kwd_replace.add_keyword(k, ' ')    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the keyword extraction, return FIPS \n",
    "def handler(data):\n",
    "    # Ignoring ads targeted to all of USA\n",
    "    if data == 'USA':\n",
    "        return data\n",
    "    \n",
    "    f = set()\n",
    "    \n",
    "    # Get the first batch of keywords\n",
    "    # City/State, District, County\n",
    "    keys = kwd.extract_keywords(data)\n",
    "    \n",
    "    # Remove keywords already found  \n",
    "    data = kwd_replace.replace_keywords(data)\n",
    "    # Get keywords of only cities \n",
    "    keys += kwd_cities.extract_keywords(data)\n",
    "    # Get keywords of only states\n",
    "    keys += kwd_states.extract_keywords(data)\n",
    "    \n",
    "    for key in keys:\n",
    "        res = get_fips(key)\n",
    "        if type(res) == list:\n",
    "            f |= set(res)\n",
    "        else:\n",
    "            f.add(res)\n",
    "            \n",
    "    if None in f:\n",
    "        f.remove(None)\n",
    "    \n",
    "    # Return as a string delimited by comma\n",
    "    new_string = ', '.join(f)\n",
    "    return new_string\n",
    "\n",
    "def get_fips(value):\n",
    "    \"\"\"\n",
    "    If zipcode isn't in the list, check the zipcode package\n",
    "    and get the county name - lookup FIPS of county name\n",
    "    \"\"\"\n",
    "    if value[0] == 'Zipcode':\n",
    "        fips = fips_dict.get(value[1])\n",
    "        if not fips:\n",
    "            county = zipcode.isequal(value[1]).county\n",
    "            return fips_dict.get(county)\n",
    "    \n",
    "    # Ignoring cities for now - its ambiguous\n",
    "    if value[0] == 'City':\n",
    "        return value[1]\n",
    "    \n",
    "    return fips_dict.get(value[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent cleaning up:  14.37647533416748\n",
      "Time spent parsing:  155.04773712158203\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "df1 = pd.read_csv(ads_file)\n",
    "# df1 = pd.read_csv('ads_file_cleaned', nrows=100)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df1[geotarget] = df1[geotarget].apply(cleanup_states)\n",
    "df1 = cleanup(df1)\n",
    "end = time.time()\n",
    "print(\"Time spent cleaning up: \",end - start)\n",
    "\n",
    "start = time.time()\n",
    "df1[geotarget] = df1[geotarget].apply(handler)\n",
    "end = time.time()\n",
    "print(\"Time spent parsing: \",end - start)\n",
    "# df1.to_csv('Ads-Filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('Ads-Filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('Ads-Filtered.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
